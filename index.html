<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Virtual Correspondence</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://people.csail.mit.edu/weichium/virtual-correspondence/img/good-will-kobe-4.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="2130">
    <meta property="og:image:height" content="976">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://people.csail.mit.edu/weichium/virtual-correspondence/"/>
    <meta property="og:title" content="Virtual Correspondence: Humans as a Cue for Extreme-View Geometry" />
    <meta property="og:description" content="Recovering the spatial layout of the cameras and the geometry of the scene from extreme-view images is a longstanding challenge in computer vision. Prevailing 3D reconstruction algorithms often adopt the image matching paradigm and presume that a portion of the scene is co-visible across images, yielding poor performance when there is little overlap among inputs. In contrast, humans can associate visible parts in one image to the corresponding invisible components in another image via prior knowledge of the shapes.   Inspired by this fact, we present a novel concept called virtual correspondences (VCs). VCs are a pair of pixels from two images whose camera rays intersect in 3D.  Similar to classic correspondences, VCs conform with epipolar geometry; unlike classic correspondences, VCs do not need to be co-visible across views. Therefore VCs can be established and exploited even if images do not overlap. We introduce a method to find virtual correspondences based on humans in the scene. We showcase how VCs can be seamlessly integrated with classic bundle adjustment to recover camera poses across extreme views.  Experiments show that our method significantly outperforms state-of-the-art camera pose estimation methods in challenging scenarios and is comparable in the traditional densely captured setup.  Our approach also unleashes the potential of multiple downstream tasks such as scene reconstruction from multi-view stereo and novel view synthesis in extreme-view scenarios."/>

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Virtual Correspondence: Humans as a Cue for Extreme-View Geometry" />
    <meta name="twitter:description" content="Recovering the spatial layout of the cameras and the geometry of the scene from extreme-view images is a longstanding challenge in computer vision. Prevailing 3D reconstruction algorithms often adopt the image matching paradigm and presume that a portion of the scene is co-visible across images, yielding poor performance when there is little overlap among inputs. In contrast, humans can associate visible parts in one image to the corresponding invisible components in another image via prior knowledge of the shapes.   Inspired by this fact, we present a novel concept called virtual correspondences (VCs). VCs are a pair of pixels from two images whose camera rays intersect in 3D.  Similar to classic correspondences, VCs conform with epipolar geometry; unlike classic correspondences, VCs do not need to be co-visible across views. Therefore VCs can be established and exploited even if images do not overlap. We introduce a method to find virtual correspondences based on humans in the scene. We showcase how VCs can be seamlessly integrated with classic bundle adjustment to recover camera poses across extreme views.  Experiments show that our method significantly outperforms state-of-the-art camera pose estimation methods in challenging scenarios and is comparable in the traditional densely captured setup.  Our approach also unleashes the potential of multiple downstream tasks such as scene reconstruction from multi-view stereo and novel view synthesis in extreme-view scenarios." />
    <meta name="twitter:image" content="https://people.csail.mit.edu/weichium/virtual-correspondence/img/good-will-kobe-4.png" />

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>

    <link rel="stylesheet" href="css/model-viewer.css">
    <!-- Import the component -->
    <script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>    

</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>Virtual Correspondence</b><br> Humans as a Cue for Extreme-View Geometry</br> 
                <small>
                CVPR 2022 
                </small>
            </h2>
        </div>

    <div class="row">
        <div class="col-md-12 text-center">
            <ul class="list-inline">
                <li>
                    <a href="http://people.csail.mit.edu/weichium/">
                      Wei-Chiu Ma<sup>1,4</sup>
                    </a>
<!--                     </br>MIT, Waabi
 -->                </li>
                <li>
                    <a href="https://www.cs.toronto.edu/~ajyang/">
                        Anqi Joyce Yang<sup>2,4</sup>
                    </a>
<!--                     </br>UofT, Waabi
 -->                </li>
                <li>
                    <a href="http://shenlong.web.illinois.edu">
                      Shenlong Wang<sup>3</sup>
                    </a>
                    <!-- </br>UIUC -->
                </li><br>
                <li>
                    <a href="http://www.cs.toronto.edu/~urtasun/index.html">
                      Raquel Urtasun<sup>2,4</sup>
                    </a>
                    <!-- </br>University of Toronto, Waabi -->
                </li>
                <li>
                    <a href="https://groups.csail.mit.edu/vision/torralbalab/">
                      Antonio Torralba<sup>1</sup>
                    </a>
                    <!-- </br>MIT -->
                </li>
            </ul>
        </div>
    </div>
   <div class="row">
        <div class="col-md-12 text-center">
            <ul class="list-inline">
                <li>
                    <sup>1</sup> MIT
                </li>
                <li>
                    <sup>2</sup> UofT
                </li>
                <li>
                    <sup>3</sup> UIUC
                </li>
                <li>
                    <sup>4</sup> Waabi
                </li>                
            </ul>
        </div>
    </div>    

    <div class="row">
            <div class="col-md-4 col-md-offset-4 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="http://people.csail.mit.edu/weichium/virtual-correspondence/top.pdf">
                        <image src="img/paper-thumbnail.jpg" height="60px">
                            <h4><strong>Paper</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://youtu.be/W9odd2F2Bx4">
                        <image src="img/youtube-icon.png" height="60px">
                            <h4><strong>Video<br>(5 min)</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="">
                        <image src="img/colab-icon.png" height="60px">
                            <h4><strong>Colab<br>(coming soon)</strong></h4>
                        </a>
                    </li>
                </ul>
            </div>
    </div>    



    <div class="row">
        <div class="col-md-8 col-md-offset-2">
            <video id="v0" width="100%" autoplay loop muted controls>
              <source src="img/vc_demo.mp4" type="video/mp4" />
            </video>
                    </div>
        <div class="col-md-8 col-md-offset-2">
                        <p class="text-center">
                        Play with sound. Video credit: Tom Buehler and Wei-Chiu Ma.
                        </p>
                    </div>
    </div>    

    <div class="row">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                Can you tell the relationships between these matched pixels?
            </h3>
            <img src="img/good-will-kobe-4.png"  width="100%">
            <p class="text-justify">The head pixel and the face pixel in the leftmost images have completely different semantics and appearances, yet we can still associate them for 3D reasoning. 
            Why, and how? 
            In this paper, we present a novel concept to establish geometric relationships between pixels even if they are not semantically or visually similar.
            </p>
        </div>
    </div>    

    <div class="row">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                Abstract
            </h3>
            <p class="text-justify">
            Recovering the spatial layout of the cameras and the geometry of the scene from extreme-view images is a longstanding challenge in computer vision. Prevailing 3D reconstruction algorithms often adopt the image matching paradigm and presume that a portion of the scene is co-visible across images, yielding poor performance when there is little overlap among inputs.  
            In contrast, humans can associate visible parts in one image to the corresponding invisible components in another image via prior knowledge of the shapes.   Inspired by this fact, we present a novel concept called virtual correspondences (VCs). VCs are a pair of pixels from two images whose camera rays intersect in 3D.  Similar to classic correspondences, VCs conform with epipolar geometry; unlike classic correspondences, VCs do not need to be co-visible across views. Therefore VCs can be established and exploited even if images do not overlap. 
            We introduce a method to find virtual correspondences based on humans in the scene. We showcase how VCs can be seamlessly integrated with classic bundle adjustment to recover camera poses across extreme views.  Experiments show that our method significantly outperforms state-of-the-art camera pose estimation methods in challenging scenarios and is comparable in the traditional densely captured setup.  Our approach also unleashes the potential of multiple downstream tasks such as scene reconstruction from multi-view stereo and novel view synthesis in extreme-view scenarios.
            </p>
        </div>
    </div>

    <div class="row">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                Classic Correspondence <em>vs.</em> Virtual Correspondence
            </h3>
            <img src="img/bunny-new-2.png"  width="100%">
            <!-- <p class="text-justify"> </p> -->
        </div>
    </div>      


    <div class="row">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                Established Correspondences across Frames
            </h3>
            <p class="text-center">Classic Correspondences (SuperGlue)</p>
            <p align="center">
            <img src="img/gif/sg.gif" width="80%">
            </p>
            <p class="text-center">Virtual Correspondences</p>
            <p align="center">
            <img src="img/gif/vc.gif" width="80%">
            </p>
            <p class="text-justify">Our system takes the best of both worlds! We combine two types of correspondences and are able to produce accurate pose estimation across a wide range of scenarios. See Sec. 4 of the main paper for more details. </p>
        </div>
    </div>     

    <div class="row">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                Pipeline
            </h3>
            <img src="img/vc-pipeline-new.png"  width="100%">
            <p class="text-justify">We first predict the 3D shape and pose of the basketball player from the left image. Then we cast a ray and record all the points it hits, <em>i.e.</em>, the belly button and his back. While the two images barely overlap, the right image does observe the back of player. We can thus tell that the rays of the two pixels intersect at 3D and are virtual correspondences.</p>
        </div>
    </div>     


    <!-- <hr> -->
    <div class="row">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                Interactive 3D Reconstruction Results
            </h3>
            <p class="text-center">CMU Panoptic Studio</p>
            <img src="img/qual_vc/00011456/171204_pose6_00_29_00011456.jpg" width=184>
            <img src="img/qual_vc/00011456/171204_pose6_00_01_00011456.jpg" width=184>
            <img src="img/qual_vc/00011456/171204_pose6_00_13_00011456.jpg" width=184>
            <img src="img/qual_vc/00011456/171204_pose6_00_27_00011456.jpg" width=184>
            <img src="img/qual_vc/00011456/171204_pose6_00_09_00011456.jpg" width=184>
            <img src="img/qual_vc/00011456/171204_pose6_00_03_00011456.jpg" width=184>
            <img src="img/qual_vc/00011456/171204_pose6_00_25_00011456.jpg" width=184>
            <img src="img/qual_vc/00011456/171204_pose6_00_07_00011456.jpg" width=184>
            <p class="text-center">
            We do not show VCs here to avoid clutter.
            </p>            
            <!-- <br> -->
            <model-viewer src="img/glb/eight-cam.glb" interaction-prompt="when-focused" camera-controls exposure="0.72" shadow-intensity="2.7" shadow-softness="0.84" min-camera-orbit="auto auto auto" max-camera-orbit="auto auto 11.89m" camera-orbit="309.2deg 86.87deg 9.5m" field-of-view="5deg" width=900>
            </model-viewer> 
            <br>
            <p class="text-center">CMU Panoptic Studio</p>            
            <img src="img/qual_vc/00031435/171204_pose4_00_01_00031435.jpg" width=184>
            <img src="img/qual_vc/00031435/171204_pose4_00_17_00031435.jpg" width=184>
            <img src="img/qual_vc/00031435/171204_pose4_00_05_00031435.jpg" width=184>
            <img src="img/qual_vc/00031435/171204_pose4_00_23_00031435.jpg" width=184>
            <img src="img/qual_vc/00031435/171204_pose4_00_07_00031435.jpg" width=184>
            <img src="img/qual_vc/00031435/171204_pose4_00_19_00031435.jpg" width=184>
            <img src="img/qual_vc/00031435/171204_pose4_00_29_00031435.jpg" width=184>
            <img src="img/qual_vc/00031435/171204_pose4_00_09_00031435.jpg" width=184>
            <p class="text-center">
            We do not show VCs here to avoid clutter.
            </p>  
            <!-- <br> -->
            <model-viewer src="img/glb/eight-cam-2.glb" interaction-prompt="when-focused" camera-controls exposure="0.72" shadow-intensity="2.7" shadow-softness="0.84" min-camera-orbit="auto auto auto" max-camera-orbit="auto auto 11.89m" camera-orbit="309.2deg 86.87deg 9.5m" field-of-view="5deg" width=900>
            </model-viewer>  
            <br>
            <p class="text-center">Mannequin Challenge (Teachers)</p>
            <img src="img/qual_vc/teacher.png" width="100%">
            <br><br>
            <model-viewer src="img/glb/teacher.glb" interaction-prompt="when-focused" camera-controls exposure="0.72" shadow-intensity="2.7" shadow-softness="0.84" min-camera-orbit="auto auto auto" max-camera-orbit="auto auto 11.89m" camera-orbit="309.2deg 86.87deg 6.5m" field-of-view="5deg" width=900>
            </model-viewer>   
            <br>
            <p class="text-center">Mannequin Challenge (Girls)</p>
            <img src="img/qual_vc/table_girl.png" width="100%">
            <br><br>
            <model-viewer src="img/glb/table_girl.glb" interaction-prompt="when-focused" camera-controls exposure="0.72" shadow-intensity="2.7" shadow-softness="0.84" min-camera-orbit="auto auto auto" max-camera-orbit="auto auto 11.89m" camera-orbit="309.2deg 86.87deg 6.5m" field-of-view="5deg" width=900>
            </model-viewer>                      
            <br>   
            <p class="text-center">Friends</p>
            <img src="img/qual_vc/friends-vc.png" width="100%">
            <p class="text-center">FoV are empirically set to 25 degrees.</p>            
            <model-viewer src="img/glb/friends.glb" interaction-prompt="when-focused" camera-controls exposure="0.72" shadow-intensity="2.7" shadow-softness="0.84" min-camera-orbit="auto auto auto" max-camera-orbit="auto auto 11.89m" camera-orbit="309.2deg 86.87deg 6.5m" field-of-view="5deg" width=900>
            </model-viewer> 
            <br>
            <p class="text-center">Michael Jordan</p>
            <img src="img/qual_vc/jordan-vc.png" width="100%">
            <p class="text-center">FoV are empirically set to 25 degrees.</p>  
            <model-viewer src="img/glb/jordan.glb" interaction-prompt="when-focused" camera-controls exposure="0.72" shadow-intensity="2.7" shadow-softness="0.84" min-camera-orbit="auto auto auto" max-camera-orbit="auto auto 11.89m" camera-orbit="429.2deg 86.87deg 12.5m" field-of-view="5deg" width=900>
            </model-viewer>             
        </div>
    </div>

    <div class="row">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                Virtual Correspondence + Multi-view stereo
            </h3>
            <p class="text-center">Input: two non-overlapping videos</p>
            <!-- <img src="img/shenlong-mvs.png" width="100%"> -->
            <p align="center">
            <img src="img/shenlong-1.gif" width=200>
            <img src="img/shenlong-2.gif" width=200>           
            </p>
            <!-- <br><br> -->
            <p class="text-center">Output: Reconstructed 3D Mesh</p>
            <p align="center">
            <iframe title="Standing Man" frameborder="0" allowfullscreen mozallowfullscreen="true" webkitallowfullscreen="true" allow="autoplay; fullscreen; xr-spatial-tracking" xr-spatial-tracking execution-while-out-of-viewport execution-while-not-rendered web-share width="640" height="480" src="https://sketchfab.com/models/e632747b416146dd9445b7e7e0ce88ad/embed"> </iframe></p>

        </div>
    </div>  


    <div class="row">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                Citation (bibtex)
            </h3>
            <div class="form-group col-md-10 col-md-offset-1">
                <textarea id="bibtex" class="form-control" readonly>
@inproceedings{ma2022virtual,
    title={Virtual Correspondence: Humans as a Cue for Extreme-View Geometry},
    author={Wei-Chiu Ma and Anqi Joyce Yang and Shenlong Wang
            and Raquel Urtasun and Antonio Torralba},
    journal={CVPR},
    year={2022}
}</textarea>
            </div>
        </div>
    </div>

    <div class="row">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                Acknowledgements
            </h3>
            <p class="text-justify">
            The authors would like to thank <a href="https://siegedog.com/">Ioan Andrei Bârsan</a>, <a href="http://www.cs.toronto.edu/~manivasagam/">Siva Manivasagam</a>, and <a href="http://www.cs.toronto.edu/~kelvinwong/">Kelvin Wong</a> for their feedback on the early draft, and <a href="https://people.csail.mit.edu/lrchai/">Lucy "When I grow up I wanna be Lucy Chai" Chai</a> for final proofreading.
            The website template is borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a>.
            </p>
        </div>
    </div>    

</div>
</body>
</html>